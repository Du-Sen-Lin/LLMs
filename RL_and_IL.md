# RL&IL

# 一、RL 【Reinforcement Learning】

```python
# https://github.com/OpenRLHF/OpenRLHF, OpenRLHF 是一个基于 Ray、DeepSpeed 和 HF Transformers 构建的高性能 RLHF 框架

# 基础: 磨菇书 easyRL 
https://datawhalechina.github.io/easy-rl/
```

## 1、easyRL

### ch1:

深度强化学习 = 深度学习 + 强化学习

code:

```python
pip install gym==0.25.2 # 强化学习工具包，支持多种经典控制任务、游戏、机器人模拟
pip install pygame # 界面显示
```

### ch2:

**马尔可夫决策过程（MDP）**：

​	策略评估（policy evaluation）：当给定决策后，怎么去计算它的价值函数；

​	控制：策略迭代（policy iteration）、价值迭代（value iteration）;

**马尔可夫性质（Markov property）**: 指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。

**马尔可夫链（Markov chain）**：离散时间的马尔可夫过程。 马尔可夫链是最简单的马尔可夫过程，其状态是有限的。

**马尔可夫奖励过程（Markov reward process, MRP）**: 是马尔可夫链加上奖励函数。

**贝尔曼方程(动态规划方程)**： 当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。

```python
# 马尔可夫奖励过程 与 马尔可夫决策过程的不同：
1.马尔可夫过程/马尔可夫奖励过程的状态转移是直接决定的。比如当前状态是 s，那么直接通过转移概率决定下一个状态是什么。 s --> s'
2.马尔可夫决策过程，它的中间多了一层动作 a, 即智能体决定动作，采取动作来决定未来状态的转移。s-->a-->s'

# 马尔可夫决策过程的价值函数
Q函数：动作价值函数（action-value function）。Q 函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望。
价值函数： 对 Q 函数中的动作进行加和。

# 策略评估
已知马尔可夫决策以及要采取的策略，计算价值函数的过程就是策略评估，也就是预测当前采取的策略最终会产生多少价值。

# 预测 与 控制
预测：评估一个给定的策略，输入是马尔可夫决策过程与策略，输出是价值函数； --> 给定策略，计算价值函数

控制：搜索最佳策略，输入是马尔可夫决策过程，输出是最佳价值函数和最佳策略；--> 未知策略，找最佳价值函数和最佳策略。不可能进行策略穷举算出每种策略的价值函数。搜索最佳策略有两种常用的方法：**策略迭代** 与 **价值迭代**。
策略迭代：由两个步骤组成：策略评估和策略改进。
价值迭代：最优性原理定理。--> 即，一个问题的最优解包含其子问题的最优解。即，从某个中间状态到达终点的最优路径，其子路径也一定是从该中间状态到终点的最优路径。

# 动态规划
动态规划适合解决满足 最优子结构 和 重叠子问题。

```

### ch3: 表格型方法

策略最简单的表示是查找表（look-up table），即表格型策略（tabular policy）。使用查找表的强化学习方法称为**表格型方法（tabular method）**，如蒙特卡洛、Q学习和Sarsa。

```python
# 蒙特卡洛方法
蒙特卡洛方法使用经验平均回报（empirical mean return）的方法来估计，它不需要马尔可夫决策过程的状态转移函数和奖励函数，并且不需要像动态规划那样用自举的方法。此外，蒙特卡洛方法有一定的局限性，它只能用在有终止的马尔可夫决策过程中。

# 
```



# 二、IL【Imitation Learning】



