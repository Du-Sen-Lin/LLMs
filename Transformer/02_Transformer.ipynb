{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4029fbe5-9d05-48aa-90cd-c9c5ec3f411a",
   "metadata": {},
   "source": [
    "# Scaled Dot-product Attention\n",
    "\n",
    "Pytorch 来手工实现 Scaled Dot-product Attention\n",
    "\n",
    "首先需要将文本分词为词语 (token) 序列，然后将每一个词语转换为对应的词向量 (embedding)。Pytorch 提供了 torch.nn.Embedding 层来完成该操作，即构建一个从 token ID 到 token embedding 的映射表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ce1515-ec45-4427-8b28-934d56db0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47b0846-47cf-425f-a7d7-7d73e1528cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer 是 Hugging Face transformers 库中的一个类，能够自动选择合适的分词器。\n",
    "# 用于文本处理流程中，将自然语言文本转换为模型可以接受的输入格式。在此例中，tokenizer 将使用BERT的预训练分词规则，处理输入文本。\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f969848a-8d08-44be-9ebc-b747091369d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2051, 10029,  2066,  2019,  8612]])\n"
     ]
    }
   ],
   "source": [
    "text = \"time flies like an arrow\"\n",
    "# 默认情况下，BERT模型需要在输入文本的开头和结尾添加特殊token（如 [CLS] 和 [SEP]）。设置add_special_tokens=False禁用了这个行为，因此输出不包括这些特殊token。\n",
    "# 每个单词被分成对应的ID，并表示成张量格式，可以直接用于模型的输入。\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f09663-d1d5-46a0-9305-eebdcac0ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(30522, 768)\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "# nn.Embedding 是 PyTorch 中用于创建词嵌入的层。\n",
    "# config.vocab_size：表示模型的词汇表大小。这个值决定了嵌入层的输入维度，即可以处理多少个不同的token（词汇）\n",
    "# config.hidden_size：表示模型的隐藏层大小，也即每个词向量的维度。在 BERT 中，这通常是 768 或 1024 等。\n",
    "# token_emb 是创建好的词嵌入层，它将整数（表示词汇表中的词）映射为对应的高维向量。\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "print(token_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c0bf8bf-54f5-484a-991e-0711fc0b0da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.19.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83482c2b-777a-415a-9981-5e715ff5fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# 输入的token ID被转换为相应的嵌入向量\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "print(inputs_embeds.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a431e-d12b-4cde-8458-9b0c1c67916e",
   "metadata": {},
   "source": [
    "创建 query、key、value 向量序列 ，并且使用点积作为相似度函数来计算注意力分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2c7f55-ab16-450c-83c8-02a69122f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa2e5a2-8e89-4b39-af44-000812a07285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "Q = K = V = inputs_embeds\n",
    "dim_k = K.size(-1)\n",
    "scores = torch.bmm(Q, K.transpose(1,2)) / sqrt(dim_k)\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4653ee61-0d7f-4094-97c7-94a9a4e56555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac75a425-bc98-4d61-9f73-6a8c1c4af9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "attn_outputs = torch.bmm(weights, V)\n",
    "print(attn_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de307d-5a34-4126-aa67-5daccb61f95e",
   "metadata": {},
   "source": [
    "此就实现了一个简化版的 Scaled Dot-product Attention。可以将上面这些操作封装为函数以方便后续调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97dab4c2-e6e8-42a1-927e-f49e13c5f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    if query_mask is not None and key_mask is not None:\n",
    "        mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -float(\"inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e424c73-00c4-4519-b0f4-cbcd95930031",
   "metadata": {},
   "source": [
    "上面的做法会带来一个问题：当 \n",
    " 和 \n",
    " 序列相同时，注意力机制会为上下文中的相同单词分配非常大的分数（点积为 1），而在实践中，相关词往往比相同词更重要。例如对于上面的例子，只有关注“time”和“arrow”才能够确认“flies”的含义。\n",
    "\n",
    "因此，多头注意力 (Multi-head Attention) 出现了！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1303d7b9-ee22-4cb0-99e9-50ceb58e7ea0",
   "metadata": {},
   "source": [
    "# Multi-head Attention(多头注意力)\n",
    "Multi-head Attention 首先通过线性映射将 Q, K, V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention：\n",
    "\n",
    "每个注意力头负责关注某一方面的语义相似性，多个头就可以让模型同时关注多个方面。因此与简单的 Scaled Dot-product Attention 相比，Multi-head Attention 可以捕获到更加复杂的特征信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fa7f5ad-3925-49f0-8145-e357fcdf9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 每个头都会初始化三个独立的线性层，负责将 Q, K, V 序列映射到尺寸为 [batch_size, seq_len, head_dim] 的张量，其中 head_dim 是映射到的向量维度。\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(query), self.k(key), self.v(value), query_mask, key_mask, mask)\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccf7f7-7dd8-4e66-8e41-b1c4364b64d7",
   "metadata": {},
   "source": [
    "实践中一般将 head_dim 设置为 embed_dim 的因数，这样 token 嵌入式表示的维度就可以保持不变，例如 BERT 有 12 个注意力头，因此每个头的维度被设置为  768/12=64\n",
    "\n",
    "最后只需要拼接多个注意力头的输出就可以构建出 Multi-head Attention 层了（这里在拼接后还通过一个线性变换来生成最终的输出张量）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89308f67-5a0c-408c-b398-f8d34d8e0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None):\n",
    "        x = torch.cat([\n",
    "            h(query, key, value, query_mask, key_mask, mask) for h in self.heads\n",
    "        ], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ab320-a4d4-40a9-a1dd-4f03ac039ac1",
   "metadata": {},
   "source": [
    "使用 BERT-base-uncased 模型的参数初始化 Multi-head Attention 层，并且将之前构建的输入送入模型以验证是否工作正常："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e737c1c-a5dd-41c8-a49c-b33e0e77efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "text = \"time flies like an arrow\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "inputs_embeds = token_emb(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ee9dc8-b802-43fd-b0a9-85b63923f4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6861245f-89ec-4c9b-9430-0ba723dc9872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "query = key = value = inputs_embeds\n",
    "attn_output = multihead_attn(query, key, value)\n",
    "print(attn_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f52a89-0837-45bb-b10c-5a4c23e9d095",
   "metadata": {},
   "source": [
    "# Transformer Encoder\n",
    "标准 Transformer 结构，Encoder 负责将输入的词语序列转换为词向量序列，Decoder 则基于 Encoder 的隐状态来迭代地生成词语序列作为输出，每次生成一个词语。\n",
    "\n",
    "输入的词语首先被转换为词向量。由于注意力机制无法捕获词语之间的位置关系，因此还通过 positional embeddings 向输入中添加位置信息；\n",
    "\n",
    "Encoder 由一堆 encoder layers (blocks) 组成，类似于图像领域中的堆叠卷积层。同样地，在 Decoder 中也包含有堆叠的 decoder layers；\n",
    "\n",
    "Encoder 的输出被送入到 Decoder 层中以预测概率最大的下一个词，然后当前的词语序列又被送回到 Decoder 中以继续生成下一个词，重复直至出现序列结束符 EOS 或者超过最大输出长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4023589-3403-43af-a86e-e07e97531f5a",
   "metadata": {},
   "source": [
    "# The Feed-Forward Layer\n",
    "Transformer Encoder/Decoder 中的前馈子层实际上就是两层全连接神经网络，它单独地处理序列中的每一个词向量，也被称为 position-wise feed-forward layer。常见做法是让第一层的维度是词向量大小的 4 倍，然后以 GELU 作为激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba83d4b2-c1cb-4d9d-9c23-bff631f4a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "189e6152-91c2-4bb1-a727-2939a9849f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_output)\n",
    "print(ff_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cc4ac-d74c-4f98-a40d-3de05cdd042b",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "\n",
    "Layer Normalization 负责将一批 (batch) 输入中的每一个都标准化为均值为零且具有单位方差；Skip Connections 则是将张量直接传递给模型的下一层而不进行处理，并将其添加到处理后的张量中。\n",
    "\n",
    "向 Transformer Encoder/Decoder 中添加 Layer Normalization 目前共有两种做法：\n",
    "\n",
    "    (1) Post layer normalization：Transformer 论文中使用的方式，将 Layer normalization 放在 Skip Connections 之间。 但是因为梯度可能会发散，这种做法很难训练，还需要结合学习率预热 (learning rate warm-up) 等技巧；\n",
    "    (2) Pre layer normalization：目前主流的做法，将 Layer Normalization 放置于 Skip Connections 的范围内。这种做法通常训练过程会更加稳定，并且不需要任何学习率预热。\n",
    "\n",
    "采用第二种方式来构建 Transformer Encoder 层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "192404c9-14fd-40f8-959a-cc3910586ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0eaccaa-73d4-45b6-aa6f-88739955fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "print(inputs_embeds.shape)\n",
    "print(encoder_layer(inputs_embeds).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d42f9e-803e-4243-b8c0-6911f4344e66",
   "metadata": {},
   "source": [
    "# Positional Embeddings\n",
    "由于注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。\n",
    "\n",
    "Positional Embeddings 基于一个简单但有效的想法：使用与位置相关的值模式来增强词向量。\n",
    "\n",
    "如果预训练数据集足够大，那么最简单的方法就是让模型自动学习位置嵌入。下面本章就以这种方式创建一个自定义的 Embeddings 模块，它同时将词语和位置映射到嵌入式表示，最终的输出是两个表示之和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f80978ae-273f-462b-bf2e-55fa0f060088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "embedding_layer = Embeddings(config)\n",
    "print(embedding_layer(inputs.input_ids).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcde63-a393-455a-b32e-58996db479af",
   "metadata": {},
   "source": [
    "除此以外，Positional Embeddings 还有一些替代方案：\n",
    "\n",
    "绝对位置表示：使用由调制的正弦和余弦信号组成的静态模式来编码位置。 当没有大量训练数据可用时，这种方法尤其有效；\n",
    "\n",
    "相对位置表示：在生成某个词语的词向量时，一般距离它近的词语更为重要，因此也有工作采用相对位置编码。因为每个词语的相对嵌入会根据序列的位置而变化，这需要在模型层面对注意力机制进行修改，而不是通过引入嵌入层来完成，例如 DeBERTa 等模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c97ed-01f7-481f-a494-33f19e29ab97",
   "metadata": {},
   "source": [
    "完整的 Transformer Encoder："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0190651-7e86-4bed-a916-92bac7816ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f2c2ecd-a5da-4387-9004-05f00995de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "print(encoder(inputs.input_ids).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6100641-3ef6-4ec6-9cbf-22ef3e102ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2051, 10029,  2066,  2019,  8612]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13e764-91ed-4b77-976b-125bd42defc8",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n",
    "Transformer Decoder 与 Encoder 最大的不同在于 Decoder 有两个注意力子层.\n",
    "\n",
    "**Masked multi-head self-attention layer**：确保在每个时间步生成的词语仅基于过去的输出和当前预测的词，否则 Decoder 相当于作弊了；\n",
    "\n",
    "**Encoder-decoder attention layer**：以解码器的中间表示作为 queries，对 encoder stack 的输出 key 和 value 向量执行 Multi-head Attention。通过这种方式，Encoder-Decoder Attention Layer 就可以学习到如何关联来自两个不同序列的词语，例如两种不同的语言。 解码器可以访问每个 block 中 Encoder 的 keys 和 values。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0de40-f3cb-48d2-b57a-6fa07b4b574a",
   "metadata": {},
   "source": [
    "与 Encoder 中的 Mask 不同，Decoder 的 Mask 是一个下三角矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62058f76-c930-4a90-a5b4-eecad157de3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "seq_len = inputs.input_ids.size(-1)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "print(mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d86e0-09a4-4b51-bec1-c7043240fe85",
   "metadata": {},
   "source": [
    "通过 Tensor.masked_fill() 将所有零替换为负无穷大来防止注意力头看到未来的词语而造成信息泄露："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2cb26d1-5b39-4f10-a3d7-fb63559717c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[25.7789,    -inf,    -inf,    -inf,    -inf],\n",
       "         [-0.4678, 28.2731,    -inf,    -inf,    -inf],\n",
       "         [-1.1618,  0.5066, 27.4563,    -inf,    -inf],\n",
       "         [ 0.8465,  0.2433,  1.1103, 29.1419,    -inf],\n",
       "         [ 0.1589, -1.4914, -0.8852,  0.8658, 28.3637]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill(mask == 0, -float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c63f9-4065-45a3-a133-60a02732f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
